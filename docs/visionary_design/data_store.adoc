= Data Store and Cache Subsystem Design
Liguang Xie <lxie@futurewei.com>
v0.1, 2019-10-27
:toc: right

[width="100%",options="header"]
|====================
|Title|Authors|Version
|Data Store and Cache Subsystem Design|@xieus|0.1
|====================

== Overview

[.lead]
Choosing the right data store system is always the key of developing any data-intensive application including Alcor control plane.
The choice is not that obvious though.
There are so many database and cache systems in the market with various characteristics as they are designed to
meet different requirements by different applications.

In this design spec, we go through our system requirements including scalability, availability,
durability, and performance.
Secondly, we review existing distributed database and cache solutions, discuss their data model, license, community support and pros and cons.
We then zoom in on selective databases and compare their features, characteristics and applicable applications.
Based on the above information, we match our system requirements with the available solutions, and propose architectural design.

[#system-requirements]
== System Requirements
=== Must-have features
* Standalone without dependency on other data-center services
* Horizontal scaling along with region expansion
* Strong consistency and ACID transaction
* Cross-DC and cross-AZ geo replication
* High availability & Fault tolerance with graceful disaster handling
* Highly performant for read heavy workload while maintaining low latency for writes
* Support rolling upgrade without downtime

=== Nice-to-have features
* Built-in cache support
* Scale to 100s nodes in one _database_ cluster in proven production environment
* Cross-shard transaction (one applicable scenario is VPC peering when two VPCs could land on different data partitions).
* In-memory processing capabilities
* Support ANSI-99 SQL
* Support collocated joins and non-collocated joins
* Support secondary index
* Event watches and leases

=== Throughput and data storage requirements
Note: The requirements given below are for VPC controller services.
Similar analysis could be done for other controllers.

* Traffic pattern
** The control plane traffic pattern is expected to be READ intensive with most majority of GURD calls are READ.

* Throughput target for data storage subsystem
** The target is 1 million read/write operations per second.
** Estimated size of a middle-sized region is 500K node with 100 ports per node with consideration of multi-port container and VMs.
Thus, the estimated total number of ports is ~50 millions.
** The estimated number of port-related resources (e.g. security groups, ACL, routes, DNS records and subnets) is of the same magnitude.
Doubling the port number would make the total number of network resources at *O*(100 millions).
** The estimated rate of system concurrency is 1% including query, insert and update,
which results in the estimated throughput of 1 million read/write operations per second.

* Storage size
** The storage size of network resource depends on multiple factors including the size of VPC and subnets,
and customer-specified fields (e.g. security rules and routes).
The size varies significantly ranging from a few _KBs_ to hundreds of _KBs_.
** On average, we look into the estimated size of *O*(10 _KB_) which could be subject to adjustment.
** Given the estimated total number of resources at *O*(100 millions), we look into a system storage sized at about 1 _TB_.


== Review of Database and Cache Solutions

Based on the expected scale and throughput, we select a few popular distributed databases from different categories including distributed SQL,
key-value store, no-SQL columnar database, document store, and graph store.
We review the data model, license, community support and discuss their pros and cons.

If you are already familiar with existing solutions on the market, please skip <<ReviewDatabase>> and refer to

- <<FeatureComp>> for more deep dive of selective distributed databases and comparison of their features, characteristics and applicable applications.
- <<#architecture>> for our system design proposal.

[#ReviewDatabase]
=== Review of Popular Databases
[width="100%",cols="1,1,3,1,1,1", options="header"]
|====================
|Name|DB model and type|Pros|Cons| Adoption and Community Support | License

|Option 1: Apache Ignite <<ignite_home>>
a|
- Multi-model database supporting both key-value and SQL for modeling and accessing data
- Developed by _Java_
a|
- Strongly consistent distributed database
- Support distributed ACID transactions, SQL queries, on-disk persistence.
- Provide strong processing APIs for computing on distributed data
- Cross DC and cross geo-region support
a|
- Supported programming languages are limited - _JAVA_, _C++_ and _C#_
a|
- Top 5 Apache project by commits
- Top 3 most active Apache mailing lists
| Apache 2.0

|Option 2: ETCD <<etcd>>
a|
- No-SQL KV store
- Developed by _Go_
a|
- Strongly consistent KV store (via Raft protocol)
- Support watch of keys or directories for changes
- Cross-platform support, small binaries
a|
- Unable to scale horizontally due to lack of data sharding
- Limited data store up to a few _GB_ <<etcd_data_model>>
a|
- Great community support backed by CNCF.
- Native storage system for Kubernetes
| Apache 2.0

|Option 3: Apache Cassandra <<cassandra>>
a|
- No-SQL columnar database developed by _JAVA_
- Eventual/ tuneable consistency level for Read/Write
- Consistent hashing for mapping keys to servers/nodes
a|
- Fast write performance
- Distributed and decentralized design (Gossip peer-to-peer protocol for distributed node management)
- Cross DC and cross geo-region support
- Large-scale deployment up to over 75,000 nodes
- Flexible scheme with CQL query support
a|
- Doesn't support ACID transaction (only AID at row/partition level)
a|
- Apache open source project originally sprung out of Facebook.
- Contributors include Apple, Linkedin, Twitter.
- Apple had the biggest Cassandra instance with 75,000+ nodes and stored more than 10 petabytes of data <<cassandra_data>>
| Apache 2.0

|Option 4: ScyllaDB <<scylla>>
a|
- Cassandra-compatible wide columnar store
- Rewrite Cassandra in _C++_
- Claimed to be the fastest NoSQL database with 99% tail latency less than 1 _msec_
a|
- Highly-performant (efficiently utilizes full resources of a node and network; millions of IOPS per node)
- Highly-available (peer-to-peer, no single-point-of-failure, active-active)
- Share many features of Cassandra like horizontal scaling, tunable consistency model and built-in geo replication
a|
- Relatively low adoption rate
a|
- Open source project adopted by Comcast, Grab, Yahoo! Japan etc.
- Not donated to any open source foundation
a|
- Open source is based on Apache GPL v3.0
- Scylla Enterprise is subscription-based
- Scylla Cloud is a managed DBaaS with various pricing models including annual, monthly and hourly

|Option 5: Apache Hbase <<hbase>>
a|
- No-SQL columnar database
- Developed by _Java_
a|
- Provides Google's Bigtable-like capabilities on top of Apache Hadoop
- Offer strong consistency
- Support structured storage for large amounts of data (on top of HDFS)
a|
- Centralized master-based architecture could cause single point of failure <<hbase_cassandra>>
- Lack of query language like Cassandra
a|
- Open source project adopted by Netflix, Flipkart, Facebook etc.
- Backup by Cloudera
| Apache 2.0

|Option 6: MangoDB <<mongodb>>
a|
- No-SQL document store developed in _C++_
- Use _JSON_ alike documents to store data
a|
- Schema-free design provides flexibility and agility on various data type
//- Fields can vary from document to document and data structure can be changed over time
- multi-document ACID Transactions with snapshot isolation
- Built in high availability, horizontal scaling, and geo distribution
a|
- MapReduce implementations remain a slow process <<mangodb_compare>>
- MongoDB suffers from memory hog issues as the databases start scaling
a|
- Great community support.
- Most widely used document-oriented database (by Google, Facebook, eBay, SAP etc.)
a|
- Community edition is under Server Side Public License (SSPL) v1 after Oct. 16, 2018, otherwise Apache GPL
- Enterprise edition is supported by MongoDB, Inc.

//|Option 7: Apache CouchDB
//a|
//- No-SQL document store
//- Store data as JSON documents and uses JavaScript as query language
//| | | |

|Option 7: Neo4j <<neo4j>>
a|
- No-SQL graph database developed in _Java_
- Data stored in documents with a focus on relationship between individual documents
a|
- ACID-compliant DBMS
- Most popular graph-oriented database as of this writing.
a|
- Unsupported data sharding
| Adopted by Ebay, Walmart, NASA etc.
a|
- Community edition is under GPL v3 license.
- Enterprise edition is supported by Neo4j, Inc.
|====================

[#FeatureComp]
=== Feature Comparision among Selective Databases

[width="100%",cols="<.^,^.<,^.<,^.<,^.<",options="header"]
|====================
|Name| Apache Ignite | ETCD | Apache Cassandra | ScyllaDB

|Applicable application
| Read-intensive or mixed application <<ignite_cassandra>>
| Application requires infrequent data update (e.g. metadata) and reliable watch queries <<etcd_data_model>>
| Write-intensive application <<ignite_cassandra>>
|

|Distributed design| Yes | Yes | Yes | Yes

|Data sharding| Yes (via distributed hashing table)
|No (data sharding unsupported)
|Yes
| Yes

|Strong consistency
|Yes
|Yes (consensus achieved through raft protocol)
| No. Eventual/tuneable consistency
| No. Eventual/tuneable consistency

|ACID transaction
|Yes (distributed transaction via improved 2-phase commit)
|Yes (single shard ACID)
|No. Light-weighted transaction (LWT)
|

|Cross-shard transaction
|Yes (with the support of transaction coordinator)
|No
|No
|No

|Concurrency modes|
Pessimistic & optimistic| | |

|Isolation levels
|Read Committed & Repeatable Read & Serializable|
|
|

|Multiversion Concurrency Control
| Yes (Snapshot isolation is in Beta at v2.7,
only support pessimistic concurrency and Repeatable Read isolation)
| Yes (A multiversion persistent & immutable kv store with past versions of key-value pair preserved and watchable)
|
|

|Data persistence
| Support WAL and check pointing
| Data stored in a persistent b+ tree
|
|

|In-memory cache capabilities
| Yes (data and indexes stored in managed off-heap regions in RAM and outside of Java heap)
| No
| No
| Yes

|ANSI-99 SQL
| Yes (via ODBC/JDBC APIs to Ignite, including both DDL and DML)
| No.
| No but support SQL-like DML and DDL statements (CQL)
| No but support SQL-like DML and DDL statements (CQL)

|Collocated joins| Yes | | No?
|

|Non-collocated Joins
a| Yes
|
| No?
|

|Geo replication
| Yes * (active-passive and active-active bi-directional replication)
| No
| Yes
|

|Secondary index | Yes | |  | Yes

|Foreign keys | No | | No | No

|Event watches/leases/elections
| Yes (cache interceptors and events)
| Yes (built-in support)
| No
|

|Synchronous replication model
|
|Single leader
|Use Zookeeper for leader election
|

|Semi-synchronous
|
|Single leader
|
|

|Replication logs and mechanism
|Write-ahead log
|Write-ahead log
|
|

|Rolling upgrade
|Enterprise edition support rolling upgrade for minor and maintenance versions of the same major series*
|
|
|

|Maximum reliable database size
|
|Several gigabytes <<etcd_data_model>>
|Apple had the biggest Cassandra instance with 75,000+ nodes and stored more than 10 petabytes of data <<cassandra_data>>
|

|====================

Note: * means that the feature is available only in the enterprise edition.

=== Review of Cache Engine

[width="100%",options="header"]
|====================
|Cache|Type|Pros|Cons|License
|Option 1: Memcache | Cache service|
|
|

|Option 2: Redis
| Cache service
a|
- Support HA cluster
- Data persistence
- Support a variety of data structures ranging from bitmaps, steams, and spatial indexes
|
| BSD

|Option 3: LevelDB | In-memory cache | | |

|Option 4: Riak
| Distributed key-value database
a|
- Distributed design
- Advanced local and multi-cluster replication
|
|
|====================

Note: Cache is optional at this point.
Our plan is to first conduct a performance analysis for various database storage solutions in terms of throughput, latency and other factors.
If TPS couldn't satisfy our target performance requirement, we will incorporate cache in our design.

=== Cache Access Pattern

Cache Aside Pattern: For write operation, we could use cache aside pattern which recommends to delete cache entry,
instead of resetting cache entry.

Pending item:

* Modify database then remove cache entry (to reduce the possibility of read old data immediate after write and legacy cache)
* Remove cache entry then modify database (ensure atomic operation)


[#architecture]
== Architectural Design

//Look-aside caching pattern for more application control
//
//=== Design Principles
//
//* High availability
//* High read/write throughput (measured in RPS)
//** Add index in read database
//** Use redundant database (for read or write, RW split, or shadow master) to improve HA and increase throughput
//** Add cache
//
//* Consistency
//** Use middleware to read from master in the inconsistent window
//** Read/Write from the same master, and add a shadow master
//
//* Extensibility

=== Data Sharding

As a result of the estimated throughput and storage size, a single machine (or even a partition with multiple replica) is
certainly unable to scale to the required high load.
In order to scale

scalability and reduced the impacted

Requirement:

* Horizontal scaling along with region expansion
* Strong consistency and ACID transaction


==== Service-aware partitioning

[width="100%",options="header"]
|====================
|Micro-Service|Partition Key|Note
|Private IP Allocator|Subnet Id| Subnet-level uniqueness
|Virtual Mac Allocator|MAC address prefix| Regional uniqueness
|Virtual IP Allocator|IP address prefix (Ipv6 and Ipv4)| Global uniqueness
|VPC Manager|VPC Id| Manage VNI/Route/ACL/Security Group
|DNS Manager|DNS record id|
|Node Manager|Node Id|
|====================

==== Data Routing Algorithm
[width="100%",options="header"]
|====================
|Data Routing Option|Pros|Cons
|Option 1: Key Range
|Simple and easy to expand
|Uneven load distribution

|Option 2: Hash by Key
|Simple and even load distribution
|Hard to migrate data during database scale-out

|Option 3: Router-config-server
|Flexible with decoupling of business logic with routing algorithm
|Additional query before every database visit

|Option 4: Embed partition information in resource id
|Simple and consistent mapping during database scale-out
|
|====================


=== Data Replication

Data replication is very useful in terms of availability and performance.

- To increase availability and resilience
- To keep data geographically close to the controller services thus reduce latency
- To increase the read throughput

Requirement:

* Cross-DC and cross-AZ geo replication
* Highly performant for read heavy workload while maintaining low latency for writes

Leader-based replication

Popular algorithms for replicating changes between nodes:

- single leader
- multi leader
- leaderless

Synchronous vs Asynchronous replication

- configurable option or hardcoded
- semi-synchronous

Alcor Replication model

- Each AZ has a primary
- Semi-synchronous replication within a AZ
- Asynchronuous replication

=== High availability

* High availability
* Fault tolerance with graceful disaster handling
** Capable of handling node outages and planned maintenance
** Zero downtime: keep the system as a whole running despite individual node failure

=== Data Inconsistency Handling

There is synchronization latency between multiple database instances (from leader to follower nodes).
This could potentially cause inconsistency in the following scenarios:

* Service instance X issues a write/update request to port
* Service instance Y requests a read/get of the same port, and the request reaches a follower node
before the synchronization is completed.
Therefore the data retrieved by instance Y is legacy data.
* Database synchronization is completed eventually

We consider three options as follows to handle such a scenario:
[width="100%",options="header"]
|====================
|Cache|Pros|Cons
|Option 1: Ignore differences | Simple working solution for many online services like web searching, message system etc.| Not applicable to scenarios requiring strong consistency
|Option 2: Read/write goes to a HA master | Common strategy used in microservice design to avoid inconsistency issue| Heavy-loaded master node with limited read throughput. Usually cache is supported to increase the read TPS.
|Option 3: Selectively reading master in the transition period| A balanced strategy: Prevent inconsistency issue in most cases and avoid overloading master node | Overhead of reading cache before database
|====================

Details about option 3:

* Write Steps

** Write to the master node
** Generate a cache key with the following format "db:table:PK" by aggregating db, table name and id
** Write to a cache and set the entry expiration time as the synchronization latency. e.g. 500 _ms_.

* Read Steps
** Use the same step to generate the cache key
** When hitting a cache, read the data from master node
** Otherwise, read the data from other nodes

=== Capacity Planning

Assuming that we have two

* Modify data routing configuration


[bibliography]
== References

- [[[ignite_home,1]]] Apache Ignite: https://ignite.apache.org/
- [[[etcd,2]]] ETCD: https://etcd.io
- [[[etcd_data_model,3]]] ETCD data model: https://github.com/etcd-io/etcd/blob/master/Documentation/learning/data_model.md
- [[[cassandra,4]]] Apache Cassandra: http://cassandra.apache.org/
- [[[cassandra_data,5]]] Apache Cassandra: Four Interesting Facts https://www.datastax.com/blog/2019/03/apache-cassandratm-four-interesting-facts
- [[[scylla,6]]] Scylla DB: https://www.scylladb.com/
- [[[hbase,7]]] Apache HBase: https://hbase.apache.org/
- [[[hbase_cassandra,8]]] Cassandra vs. HBase: twins or just strangers with similar looks? https://www.scnsoft.com/blog/cassandra-vs-hbase
- [[[mongodb,9]]] MangoDB: https://www.mongodb.com/
- [[[mangodb_compare, 10]]] Cassandra vs. MongoDB vs. Hbase: A Comparison of NoSQL Databases https://logz.io/blog/nosql-database-comparison/
- [[[neo4j,11]]] Neo4j: http://neo4j.com
- [[[ignite_cassandra,12]]] Apache Ignite and Apache Cassandra Benchmarks: The Power of In-Memory Computing (https://www.gridgain.com/resources/blog/apacher-ignitetm-and-apacher-cassandratm-benchmarks-power-in-memory-computing)
