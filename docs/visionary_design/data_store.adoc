= Data Store and Cache Subsystem Design

[width="100%",options="header"]
|====================
|Title|Authors|Version
|Data Store and Cache Subsystem Design|@xieus|0.1
|====================

== System Requirements
=== Must-have features
* Standalone without dependency on other data-center services
* Horizontal scaling along with region expansion
* Strong consistency and ACID transaction
* Cross-DC and cross-AZ geo replication
* High availability & Fault tolerance with graceful disaster handling
* Highly performant for read heavy workload while maintaining low latency for writes

=== Nice-to-have features
* Scale to 100s nodes in one _database_ cluster in proven production environment
* Cross-shard transaction (one applicable scenario is VPC peering when two VPCs could land on different data partitions).
* In-memory processing capabilities
* Support ANSI-99 SQL
* Support collocated joins and non-collocated joins
* Support secondary index
* Event watches and leases

=== Throughput and data storage requirements
Note: The requirements given below are for VPC controller services. Similar analysis could be done for other controllers.

* Traffic pattern
** The control plane traffic pattern is expected to be READ intensive with most majority of GURD calls are READ.

* Throughput target for data storage subsystem
** The target is 1 million read/write operations per second.
** Estimated size of a middle-sized region is 500K node with 100 ports per node with consideration of multi-port container and VMs.
Thus, the estimated total number of ports is ~50 millions.
** The estimated number of port-related resources (e.g. security groups, ACL, routes, DNS records and subnets) is of the same magnitude.
Doubling the port number would make the total number of network resources at *O*(100 millions).
** The estimated rate of system concurrency is 1% including query, insert and update,
which results in the estimated throughput of 1 million read/write operations per second.

* Storage size
** The storage size of network resource depends on multiple factors including the size of VPC and subnets,
and customer-specified fields (e.g. security rules and routes).
The size varies significantly ranging from a few _KBs_ to hundreds of _KBs_.
** On average, we look into the estimated size of *O*(10 _KB_) which could be subject to adjustment.
** Given the estimated total number of resources at *O*(100 millions), we look into a system storage sized at about 1 _TB_.

== Architectural Design

//Look-aside caching pattern for more application control
//
//=== Design Principles
//
//* High availability
//* High read/write throughput (measured in RPS)
//** Add index in read database
//** Use redundant database (for read or write, RW split, or shadow master) to improve HA and increase throughput
//** Add cache
//
//* Consistency
//** Use middleware to read from master in the inconsistent window
//** Read/Write from the same master, and add a shadow master
//
//* Extensibility

=== Data Sharding

As a result of the estimated throughput and storage size, a single machine (or even a partition with multiple replica) is
certainly unable to scale to the required high load.
In order to scale

scalability and reduced the impacted

==== Service-aware partitioning

[width="100%",options="header"]
|====================
|Micro-Service|Partition Key|Note
|Private IP Allocator|Subnet Id| Subnet-level uniqueness
|Virtual Mac Allocator|MAC address prefix| Regional uniqueness
|Virtual IP Allocator|IP address prefix (Ipv6 and Ipv4)| Global uniqueness
|VPC Manager|VPC Id| Manage VNI/Route/ACL/Security Group
|DNS Manager|DNS record id|
|Node Manager|Node Id|
|====================

==== Data Routing Algorithm
[width="100%",options="header"]
|====================
|Data Routing Option|Pros|Cons
|Option 1: Key Range
|Simple and easy to expand
|Uneven load distribution

|Option 2: Hash by Key
|Simple and even load distribution
|Hard to migrate data during database scale-out

|Option 3: Router-config-server
|Flexible with decoupling of business logic with routing algorithm
|Additional query before every database visit

|Option 4: Embed partition information in resource id
|Simple and consistent mapping during database scale-out
|
|====================


=== Data Replication

Data replication is very useful in terms of availability and performance.

- To increase availability and resilience
- To keep data geographically close to the controller services thus reduce latency
- To increase the read throughput

Leader-based replication

Popular algorithms for replicating changes between nodes:

- single leader
- multi leader
- leaderless

Synchronous vs Asynchronous replication

- configurable option or hardcoded
- semi-synchronous

Alcor Replication model

- Each AZ has a primary
- Semi-synchronous replication within a AZ
- Asynchronuous replication

=== Data Inconsistency Handling

There is synchronization latency between multiple database instances (from master to slave nodes).
This could potentially cause inconsistency in the following scenarios:

* Service instance X issues a write/update request to port
* Service instance Y requests a read/get of the same port, and the request reaches a slave node before the synchronization is completed.
Therefore the data retrieved by instance Y is latency data.
* Database synchronization is completed eventually

We consider three options as follows to handle such a scenario:
[width="100%",options="header"]
|====================
|Cache|Pros|Cons
|Option 1: Ignore differences | Simple working solution for many online services like web searching, message system etc.| Not applicable to scenarios requiring strong consistency
|Option 2: Read/write goes to a HA master | Common strategy used in microservice design to avoid inconsistency issue| Heavy-loaded master node with limited read throughput. Usually cache is supported to increase the read TPS.
|Option 3: Selectively reading master in the transition period| A balanced strategy: Prevent inconsistency issue in most cases and avoid overloading master node | Overhead of reading cache before database
|====================

Details about option 3:

* Write Steps

** Write to the master node
** Generate a cache key with the following format "db:table:PK" by aggregating db, table name and id
** Write to a cache and set the entry expiration time as the synchronization latency. e.g. 500 _ms_.

* Read Steps
** Use the same step to generate the cache key
** When hitting a cache, read the data from master node
** Otherwise, read the data from other nodes

=== Capacity Planning

Assuming that we have two

* Modify data routing configuration

== Review of Data Storage and Recommendation

[width="100%",options="header"]
|====================
|Name|DB model and type|Pros|Cons| License and Pricing Models| Community Support and Key Customers

|Option 1: Apache Ignite
a|
- Multi-model database supporting both key-value and SQL for modeling and accessing data
- Developed by _Java_
a|
- Strongly consistent distributed database
- Support distributed ACID transactions, SQL queries, on-disk persistence.
- Provide strong processing APIs for computing on distributed data
a|
- Supported programming languages are limited - _JAVA_, _C++_ and _C#_
| Apache 2.0
| Open Source

|Option 2: ETCD
a|
- No-SQL KV store
- Developed by _Go_
a|
- Great community support. Native storage system for Kubernetes
- Highly performant, efficient, and scalable
|
| Apache 2.0
|

|Option 3: Apache Cassandra
a|
- No-SQL columnar database developed by _JAVA_
- Eventual/tuneable consistency level for Read/Write
- Consistent hashing for mapping keys to servers/nodes
a|
- Fast write performance
- High availability due to distributed and decentralized design.
Use Zookeeper for leader election and Gossip peer-to-peer protocol for distributed node management.
- Cross DC and cross geo-region support
- Large-scale deployment up to over 75,000 nodes
- Flexible scheme with CQL query support
a|
- Doesn't support ACID transaction (only AID at row/partition level)
|
| Apache project started by Facebook. Contributors include Apple, Linkedin, Twitter.

|Option 4: ScyllaDB
a|
- Cassandra-compatible wide columnar store
- Rewrite Cassandra in _C++_
- Claimed to be the fastest NoSQL database with 99% latency of <1 msec
a|
- Highly-performant (efficiently utilizes full resources of a node and network; millions of IOPS per node)
- Highly-available (peer-to-peer, no single-point-of-failure, active-active)
- Share many features of Cassandra like horizontal scalling, tunable consistency model and built-in geo replication

|
a|
- Scylla open source is based on GPL
- Scylla Enterprise is subscription-based
- Scylla Cloud is a managed DBaaS with various pricing models including annual, monthly and hourly
| Comcast, Grab, Yahoo! Japan

|Option 5: Apache Hbase
a|
- No-SQL columnar database
a|
- Distributed design
- Support structured storage for large amounts of data
-
| | |

|Option 6: MangoDB
a|
- No-SQL document store developed in _C++_
- Use _JSON_ alike documents to store data
a|
- Great community support. Most widely used document-oriented database.
| | |

|Option 7: Apache CouchDB
a|
- No-SQL document store
- Store data as JSON documents and uses JavaScript as query language
| | | |

|Option 8: Neo4j
a|
- No-SQL graph database
- Data stored in documents with a focus on relationship between individual documents
a|
- ACID-compliant DBMS
- Most popular graph-oriented database as of this writing.
| | |

|Option 9: Azure Service Fabric
a|
a|
| | |
|====================

=== Feature Comparision among Selective Databases

[width="100%",cols="<.^,^.^,^.^,^.^,^.^",options="header"]
|====================
|Name| Apache Ignite | ETCD | Apache Cassandra | ScyllaDB

|Applicable application
| Read-intensive or mixed application <<ignite_cassandra,1>>
| Application requires infrequent data update (e.g. metadata) and reliable watch queries <<etcd_data_model,2>>
| Write-intensive application <<ignite_cassandra,1>>
|

|Distributed design| Yes | Yes | Yes | Yes

|Data partition| Yes (via distributed hashing table)
|No (data sharding unsupported)
|Yes
| Yes

|Strong consistency
|Yes
|Yes (consensus achieved through raft protocol)
| No. Eventual/tuneable consistency
| No. Eventual/tuneable consistency

|ACID transaction
|Yes (via 2-phase commit)
|
|No. Light-weighted transaction (LWT)
|

|Cross-partition transaction
|Yes with the support of transaction coordinator failover
|No
|No
|No

|Concurrency modes|
Pessimistic & optimistic| | |

|Isolation levels
|Read Committed & Repeatable Read & Serializable|
|
|

|Multiversion Concurrency Control
| Yes (Snapshot isolation is in Beta at v2.7,
only support pessimistic concurrency and Repeatable Read isolation)
| Yes (A multiversion persistent & immutable kv store with past versions of key-value pair preserved and watchable)
|
|

|Data persistence
| Support WAL and check pointing
| Data stored in a persistent b+ tree
|
|

|In-memory capabilities
| Yes (data and indexes stored in managed off-heap regions in RAM and outside of Java heap)
|
| No | Yes

|ANSI-99 SQL
| Yes (via ODBC/JDBC APIs to Ignite, including both DDL and DML)
|
| No. Support SQL-like DML and DDL statements (CQL)
| No. Support SQL-like DML and DDL statements (CQL)

|Collocated joins| Yes | | No?
|

|Non-collocated Joins
a| Yes
|
| No?
|

|Geo replication
| Yes * (active-passive and active-active bi-directional replication)
| No
| Yes
|

|Secondary index | Yes | |  | Yes

|Foreign keys | No | | No | No

|Event watches/leases/elections
| Yes (cache interceptors and events)
| Yes (built-in support)
| No
|

|Synchronous replication model
|
|Single leader
|
|


|Semi-synchronous
|
|Single leader
|
|

|Maximum reliable database size
|
|Several gigabytes <<etcd_data_model,2>>
|
|

|====================

Note: * means that the feature is available only in the enterprise edition.

== Review of Cache Engine

[width="100%",options="header"]
|====================
|Cache|Type|Pros|Cons|License
|Option 1: Memcache | Cache service|
|
|

|Option 2: Redis
| Cache service
a|
- Support HA cluster
- Data persistence
- Support a variety of data structures ranging from bitmaps, steams, and spatial indexes
|
| BSD

|Option 3: LevelDB | In-memory cache | | |

|Option 4: Riak
| Distributed key-value database
a|
- Distributed design
- Advanced local and multi-cluster replication
|
|
|====================

Note: Cache is optional at this point.
Our plan is to first conduct a performance analysis for various database storage solutions in terms of throughput, latency and other factors.
If TPS couldn't satisfy our target performance requirement, we will incorporate cache in our design.

=== Cache Aside Pattern
For write operation, we could use cache aside pattern which recommends to delete cache entry, instead of resetting cache entry.

Pending item:

* Modify database then remove cache entry (to reduce the possibility of read old data immediate after write and legacy cache)
* Remove cache entry then modify database (ensure atomic operation)


[bibliography]
== References

1. [[ignite_cassandra]] Apache Ignite and Apache Cassandra Benchmarks: The Power of In-Memory Computing (https://www.gridgain.com/resources/blog/apacher-ignitetm-and-apacher-cassandratm-benchmarks-power-in-memory-computing)
2. [[etcd_data_model]] ETCD data model (https://github.com/etcd-io/etcd/blob/master/Documentation/learning/data_model.md)